{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gTa1dvhOEFj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import corner\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "tfk = tf.keras\n",
        "\n",
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = tfk.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "y_train = tfk.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tfk.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define the prior and posterior distributions\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    return tfd.Independent(tfd.Normal(loc=tf.zeros(n), scale=1), reinterpreted_batch_ndims=1)\n",
        "\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    return tfd.Independent(tfd.Normal(loc=tf.Variable(tf.random.normal([n])),\n",
        "                                       scale=tf.Variable(tf.ones([n]))),\n",
        "                           reinterpreted_batch_ndims=1)\n",
        "\n",
        "# Define the Bayesian neural network model\n",
        "def build_bnn(input_shape, output_dim):\n",
        "    inputs = tfk.Input(shape=input_shape)\n",
        "    x = tfpl.Convolution2DFlipout(32, kernel_size=(3, 3), activation=tf.nn.relu)(inputs)\n",
        "    x = tfk.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = tfpl.Convolution2DFlipout(64, kernel_size=(3, 3), activation=tf.nn.relu)(x)\n",
        "    x = tfk.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = tfpl.Convolution2DFlipout(64, kernel_size=(3, 3), activation=tf.nn.relu)(x)\n",
        "    x = tfk.layers.Flatten()(x)\n",
        "    x = tfpl.DenseFlipout(64, activation=tf.nn.relu)(x)\n",
        "    outputs = tfpl.DenseFlipout(output_dim)(x)\n",
        "    model = tfk.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Build the Bayesian neural network\n",
        "bnn = build_bnn((28, 28, 1), 10)\n",
        "\n",
        "# Define the negative log-likelihood loss function\n",
        "def negative_log_likelihood(y_true, y_pred):\n",
        "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1))\n",
        "\n",
        "# Variational Inference\n",
        "def run_variational_inference():\n",
        "    # Define the KL divergence regularizer\n",
        "    kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
        "                              tf.cast(x_train.shape[0], dtype=tf.float32))\n",
        "\n",
        "    # Compile the model with the negative log-likelihood loss and KL divergence regularizer\n",
        "    bnn.compile(optimizer=tfk.optimizers.Adam(learning_rate=0.001),\n",
        "                loss=negative_log_likelihood,\n",
        "                metrics=[tfk.metrics.CategoricalAccuracy()],\n",
        "                experimental_run_tf_function=False)\n",
        "\n",
        "    # Train the model with variational inference\n",
        "    bnn.fit(x_train, y_train,\n",
        "            batch_size=128,\n",
        "            epochs=10,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[tfp.keras.callbacks.KLDivergenceAddLoss(kl_divergence_function)])\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_acc = bnn.evaluate(x_test, y_test)\n",
        "    print(f\"Variational Inference - Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Hamiltonian Monte Carlo\n",
        "def run_hmc():\n",
        "    # Run HMC to sample from the posterior distribution\n",
        "    num_burnin_steps = 500\n",
        "    num_results = 500\n",
        "    num_leapfrog_steps = 50\n",
        "    step_size = 0.03\n",
        "\n",
        "    # Define the joint log probability function\n",
        "    def joint_log_prob(model_params):\n",
        "        prior_log_prob = tf.reduce_sum(prior.log_prob(model_params))\n",
        "        logits = bnn(x_train)\n",
        "        log_likelihood = tf.reduce_sum(y_train * tfk.backend.log(logits), axis=-1)\n",
        "        return prior_log_prob + log_likelihood\n",
        "\n",
        "    # Initialize the HMC kernel\n",
        "    adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
        "        tfp.mcmc.HamiltonianMonteCarlo(\n",
        "            target_log_prob_fn=joint_log_prob,\n",
        "            num_leapfrog_steps=num_leapfrog_steps,\n",
        "            step_size=step_size),\n",
        "        num_adaptation_steps=num_burnin_steps)\n",
        "\n",
        "    # Run the HMC chain\n",
        "    @tf.function\n",
        "    def run_chain():\n",
        "        return tfp.mcmc.sample_chain(\n",
        "            num_results=num_results,\n",
        "            num_burnin_steps=num_burnin_steps,\n",
        "            current_state=bnn.trainable_variables,\n",
        "            kernel=adaptive_hmc)\n",
        "\n",
        "    samples, _ = run_chain()\n",
        "\n",
        "    # Make predictions using the HMC samples\n",
        "    y_pred_samples = []\n",
        "    for i in range(num_results):\n",
        "        bnn.set_weights(samples[i])\n",
        "        y_pred_samples.append(bnn(x_test))\n",
        "\n",
        "    y_pred_mean = tf.reduce_mean(y_pred_samples, axis=0)\n",
        "    y_pred_classes = tf.argmax(y_pred_mean, axis=-1)\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_classes, tf.argmax(y_test, axis=-1)), tf.float32))\n",
        "    print(f\"HMC - Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Plot the corner plot to visualize uncertainties\n",
        "    flat_samples = np.array([tf.reshape(sample, [-1]).numpy() for sample in samples])\n",
        "    fig = corner.corner(flat_samples)\n",
        "    plt.show()\n",
        "\n",
        "# Run variational inference\n",
        "run_variational_inference()\n",
        "\n",
        "# Run Hamiltonian Monte Carlo\n",
        "run_hmc()"
      ]
    }
  ]
}